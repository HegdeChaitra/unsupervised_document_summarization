{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Required Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 15 12:37:45 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P40           On   | 00000000:05:00.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    51W / 250W |      0MiB / 22919MiB |      0%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "import load_dataset_es_sample\n",
    "from load_dataset_es_sample import train_val_load\n",
    "from bleu_score import BLEU_SCORE\n",
    "\n",
    "from load_moss_dataset import *\n",
    "\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPRESSION = 0.722606950073752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "    def __init__(self, df, val = False):\n",
    "        self.df = df\n",
    "        self.val = val\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx,:]['en_idized']\n",
    "        en_len = self.df.iloc[idx,:]['en_len']\n",
    "\n",
    "        if self.val:\n",
    "            en_sum = self.df.iloc[idx,:]['ref0'].lower()\n",
    "            en_data = self.df.iloc[idx,:]['en_data'].lower()\n",
    "            return [english, en_len, en_data, en_sum]\n",
    "        else:\n",
    "            return [english, en_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "\n",
    "\n",
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)\n",
    "\n",
    "def convert_id_list_2_sent(list_idx, lang_obj):\n",
    "    word_list = []\n",
    "    if type(list_idx) == list:\n",
    "        for i in list_idx:\n",
    "            if i not in set([EOS_token]):\n",
    "                word_list.append(lang_obj.index2word[i])\n",
    "    else:\n",
    "        for i in list_idx:\n",
    "            if i.item() not in set([EOS_token,SOS_token,PAD_IDX]):\n",
    "                word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/brs426/unsupervised_document_summarization\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the correct language pkl and change the data path to the place where you store gigaword valid.article.filter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading English: test\n",
      "['input.txt' 'task1_ref0' 'task1_ref1' 'task1_ref2' 'task1_ref3'\n",
      " 'task1_ref4' 'input_len' 'ref0_len' 'ref1_len' 'ref2_len' 'ref3_len'\n",
      " 'ref4_len']\n",
      "tokenizing\n",
      "token to index mapping\n",
      "Computing length\n"
     ]
    }
   ],
   "source": [
    "test, en_lang, intermediate_lang = test_load(50, \"./it_en_lang_obj.pkl\", '/home/brs426/')\n",
    "# _, _, _, _, _, en_vecs, pi_vecs = train_val_load(48, \"\", '../../../scratch/brs426', 100000, '../../../scratch/brs426/cc.en.300.vec', '../../../scratch/brs426/cc.es.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>en_len</th>\n",
       "      <th>ref0</th>\n",
       "      <th>ref1</th>\n",
       "      <th>ref2</th>\n",
       "      <th>ref3</th>\n",
       "      <th>ref4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We believe that the European Union can make an...</td>\n",
       "      <td>[we, believe, that, the, european, union, can,...</td>\n",
       "      <td>[53, 509, 17, 5, 30493, 3863, 46, 116, 42, 136...</td>\n",
       "      <td>41</td>\n",
       "      <td>We believe the EU can make an impact , but onl...</td>\n",
       "      <td>but only if it speaks with one voice and if it...</td>\n",
       "      <td>We believe the European Union can make an impa...</td>\n",
       "      <td>We believe the EU can make an impact , but onl...</td>\n",
       "      <td>the UN can make an impact but needs proper too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The figures are worse in Africa and parts of Asia</td>\n",
       "      <td>[the, figures, are, worse, in, africa, and, pa...</td>\n",
       "      <td>[5, 2190, 28, 2269, 12, 31342, 7, 961, 9, 4315...</td>\n",
       "      <td>11</td>\n",
       "      <td>It is worse in Africa and parts of Asia</td>\n",
       "      <td>The figures are worse in Africa and parts of Asia</td>\n",
       "      <td>The figures are worse in Africa and Asia</td>\n",
       "      <td>It is worse in Africa and parts of Asia</td>\n",
       "      <td>The numbers are worse in Africa and Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The euro is the tangible result of EMU .</td>\n",
       "      <td>[the, euro, is, the, tangible, result, of, emu...</td>\n",
       "      <td>[5, 12265, 13, 5, 14792, 640, 9, 77210, 6, 1]</td>\n",
       "      <td>10</td>\n",
       "      <td>EMU results in the tangible Euro</td>\n",
       "      <td>The Euro is a tangible result .</td>\n",
       "      <td>Euro is a tangible result of EMU</td>\n",
       "      <td>EMU results in the tangible Euro</td>\n",
       "      <td>The Euro is the result of EMU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>[some, years, ago, ,]</td>\n",
       "      <td>[89, 128, 361, 4, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>Years ago</td>\n",
       "      <td>Some years ago .</td>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>Years ago ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have always believed that it is vital that B...</td>\n",
       "      <td>[i, have, always, believed, that, it, is, vita...</td>\n",
       "      <td>[202, 36, 256, 2719, 17, 23, 13, 4672, 17, 809...</td>\n",
       "      <td>24</td>\n",
       "      <td>I always that it is vital that Britain plays a...</td>\n",
       "      <td>It is vital that Great Britain play a committe...</td>\n",
       "      <td>I believe that Britain should be a central pla...</td>\n",
       "      <td>I always that it is vital that Britain plays a...</td>\n",
       "      <td>central player in the European Union</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  We believe that the European Union can make an...   \n",
       "1  The figures are worse in Africa and parts of Asia   \n",
       "2           The euro is the tangible result of EMU .   \n",
       "3                                   Some years ago ,   \n",
       "4  I have always believed that it is vital that B...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [we, believe, that, the, european, union, can,...   \n",
       "1  [the, figures, are, worse, in, africa, and, pa...   \n",
       "2  [the, euro, is, the, tangible, result, of, emu...   \n",
       "3                              [some, years, ago, ,]   \n",
       "4  [i, have, always, believed, that, it, is, vita...   \n",
       "\n",
       "                                           en_idized  en_len  \\\n",
       "0  [53, 509, 17, 5, 30493, 3863, 46, 116, 42, 136...      41   \n",
       "1  [5, 2190, 28, 2269, 12, 31342, 7, 961, 9, 4315...      11   \n",
       "2      [5, 12265, 13, 5, 14792, 640, 9, 77210, 6, 1]      10   \n",
       "3                               [89, 128, 361, 4, 1]       5   \n",
       "4  [202, 36, 256, 2719, 17, 23, 13, 4672, 17, 809...      24   \n",
       "\n",
       "                                                ref0  \\\n",
       "0  We believe the EU can make an impact , but onl...   \n",
       "1            It is worse in Africa and parts of Asia   \n",
       "2                   EMU results in the tangible Euro   \n",
       "3                                   Some years ago ,   \n",
       "4  I always that it is vital that Britain plays a...   \n",
       "\n",
       "                                                ref1  \\\n",
       "0  but only if it speaks with one voice and if it...   \n",
       "1  The figures are worse in Africa and parts of Asia   \n",
       "2                    The Euro is a tangible result .   \n",
       "3                                          Years ago   \n",
       "4  It is vital that Great Britain play a committe...   \n",
       "\n",
       "                                                ref2  \\\n",
       "0  We believe the European Union can make an impa...   \n",
       "1           The figures are worse in Africa and Asia   \n",
       "2                   Euro is a tangible result of EMU   \n",
       "3                                   Some years ago .   \n",
       "4  I believe that Britain should be a central pla...   \n",
       "\n",
       "                                                ref3  \\\n",
       "0  We believe the EU can make an impact , but onl...   \n",
       "1            It is worse in Africa and parts of Asia   \n",
       "2                   EMU results in the tangible Euro   \n",
       "3                                   Some years ago ,   \n",
       "4  I always that it is vital that Britain plays a...   \n",
       "\n",
       "                                                ref4  \n",
       "0  the UN can make an impact but needs proper too...  \n",
       "1           The numbers are worse in Africa and Asia  \n",
       "2                      The Euro is the result of EMU  \n",
       "3                                        Years ago ,  \n",
       "4               central player in the European Union  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 9)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func_test(batch):\n",
    "    return [torch.from_numpy(np.array(batch[0][0])).unsqueeze(0),\n",
    "            torch.from_numpy(np.array(batch[0][1])).unsqueeze(0), batch[0][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = {'test':1}\n",
    "shuffle_dict = {'test':False}\n",
    "\n",
    "transformed_dataset = {'test':Test(test, val= True)}\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=vocab_collate_func_test,\n",
    "                    shuffle=shuffle_dict[x], num_workers=0) for x in ['test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size,n_layers, vecs=None, rnn_type = 'lstm', device = device):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.embedding = Embedding(input_size, embed_dim, PAD_IDX)\n",
    "        # Load in fasttext vectors\n",
    "        if vecs:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vecs))\n",
    "        else:\n",
    "            self.embedding = Embedding(input_size, embed_dim, PAD_IDX)\n",
    "        self.rnn_type =  rnn_type\n",
    "        self.dropout_in = nn.Dropout(p = 0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_size,batch_first=True,bidirectional=True, num_layers = self.n_layers, dropout = 0.2)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = LSTM(embed_dim, hidden_size, batch_first=True,bidirectional=True, num_layers = n_layers,dropout = 0.2)\n",
    "\n",
    "    def forward(self, enc_inp, src_len):\n",
    "        sorted_idx = torch.sort(src_len, descending=True)[1]\n",
    "        orig_idx = torch.sort(sorted_idx)[1]\n",
    "        embedded = self.embedding(enc_inp)\n",
    "        bs = embedded.size(0)\n",
    "        output = self.dropout_in(embedded)\n",
    "        if self.rnn_type == 'gru':\n",
    "            hidden =  self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, hiddden = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, hidden\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            hidden, c = self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, (hiddden, c) = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            c = c[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            c = c.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, c\n",
    "        \n",
    "    def initHidden(self,bs):\n",
    "        if self.rnn_type == 'gru' :\n",
    "            return torch.zeros(self.n_layers*2, bs, self.hidden_size).to(self.device)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            return torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device),torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device)\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, device = device):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = Linear(hidden_dim, output_dim, bias = False)\n",
    "        self.l2 = Linear(hidden_dim+output_dim, output_dim, bias =  False)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, hidden, encoder_outs, src_lens):\n",
    "        ''' hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "        \n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_len'''\n",
    "        x = self.l1(hidden)\n",
    "        att_score = (encoder_outs.transpose(0,1) * x.unsqueeze(0)).sum(dim = 2)\n",
    "        seq_mask = sequence_mask(src_lens, max_len = max(src_lens).item(), device = self.device).transpose(0,1)\n",
    "        masked_att = seq_mask*att_score\n",
    "        masked_att[masked_att==0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0)\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0,1)).sum(dim=0)\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1)))\n",
    "        return x, attn_scores\n",
    "        \n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size, vecs=None, n_layers = 1, len_control=False, attention = True, device = device):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_output_size = hidden_size\n",
    "        # self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        # Load in fasttext vectors\n",
    "        if vecs:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vecs))\n",
    "        else:\n",
    "            self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        self.len_control = len_control\n",
    "        \n",
    "        # Create lenEmbedding if this is a length-controlled decoder\n",
    "        if len_control:\n",
    "            self.lenEmbedding = nn.Embedding(250, 100)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.att_layer = Attention_Module(self.hidden_size, encoder_output_size,self.device) if attention else None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=embed_dim + self.hidden_size + 100 if ((layer == 0) and attention and len_control) else embed_dim + self.hidden_size if ((layer == 0) and attention) else embed_dim if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(self.n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.no_len_control_ll = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.len_control_ll = nn.Linear(1536, 1024)\n",
    "#         self.len_params = nn.Parameter(torch.rand(1,512))\n",
    "        \n",
    "    \n",
    "    # Need to pass t\n",
    "    def forward(self, dec_input, context_vector, prev_hiddens, prev_cs, encoder_outputs, src_len, tar_len, init=False):\n",
    "        bsz = dec_input.size(0)\n",
    "        output = self.embedding(dec_input)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        \n",
    "        # Check if decoder is length controlled\n",
    "        # Get length embeddings and concatenate to output\n",
    "        if self.len_control:\n",
    "            tar_len[tar_len < 0] = 0\n",
    "            tar_embed = self.lenEmbedding(tar_len).unsqueeze(1)\n",
    "            output = torch.cat((output, tar_embed), dim=2)\n",
    "        \n",
    "        # Remove lenInit len_control stuff\n",
    "        if init:\n",
    "            # Initialize hidden state\n",
    "            prev_hiddens = self.no_len_control_ll(torch.mean(prev_hiddens, dim=1)).unsqueeze(0)\n",
    "#             if len_control:\n",
    "#                 prev_hiddens = self.len_control_ll(torch.cat((torch.mean(prev_hiddens, dim=1), \\\n",
    "#                                                          self.len_params.repeat(bsz, 1)*tar_len.unsqueeze(1).float()),dim=1)).unsqueeze(0)\n",
    "#                 prev_hiddens = self.len_control_ll(torch.cat((torch.mean(prev_hiddens, dim=1), \\\n",
    "#                                                            self.len_params.repeat(bsz, 1)*tar_len),dim=1)).unsqueeze(0)\n",
    "#             else:\n",
    "#                 prev_hiddens = self.no_len_control_ll(torch.mean(prev_hiddens, dim=1)).unsqueeze(0)\n",
    "            \n",
    "#         print(\"decoder\", prev_hiddens.size(), len_control)\n",
    "#         print(dec_input.size(), prev_hiddens.size(), context_vector.size(), prev_cs.size(), len_control, init)\n",
    "        if self.att_layer is not None:\n",
    "            cated_input = torch.cat([output.squeeze(1),context_vector], dim = 1)\n",
    "        else:\n",
    "            cated_input = output.squeeze(1)\n",
    "        new_hiddens = []\n",
    "        new_cs = []\n",
    "        for i, rnn in enumerate(self.layers):\n",
    "            hidden, c = rnn(cated_input, (prev_hiddens[i], prev_cs[i]))\n",
    "            cated_input = self.dropout(hidden)\n",
    "            new_hiddens.append(hidden.unsqueeze(0))\n",
    "            new_cs.append(c.unsqueeze(0))\n",
    "        new_hiddens = torch.cat(new_hiddens, dim = 0)\n",
    "        new_cs = torch.cat(new_cs, dim = 0)\n",
    "\n",
    "        # apply attention using the last layer's hidden state\n",
    "        # Issue here with lenEmbed implementation\n",
    "        if self.att_layer is not None:\n",
    "            out, attn_score = self.att_layer(hidden, encoder_outputs, src_len)\n",
    "        else:\n",
    "            out = hidden\n",
    "            attn_score = None\n",
    "        context_vec = out\n",
    "        out = self.dropout(out)\n",
    "        out_vocab = self.softmax(self.fc_out(out))\n",
    "        \n",
    "        # Return tar_len decremented if length control otherwise return nothing for target length\n",
    "        if self.len_control:\n",
    "            return out_vocab, context_vec, new_hiddens, new_cs, attn_score, tar_len - 1\n",
    "        else:\n",
    "            return out_vocab, context_vec, new_hiddens, new_cs, attn_score, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Utilites functions, training and evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTM(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTM(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True, dropout=0):\n",
    "    \"\"\"Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features, bias=bias)\n",
    "    m.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        m.bias.data.uniform_(-0.1, 0.1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None, device = 'cuda'):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.max().item()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size,1])\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return (seq_range_expand < seq_length_expand).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cel_loss(input,target,nll):\n",
    "    input = input.transpose(1,2)\n",
    "    bs, sl = input.size()[:2]\n",
    "    return nll(input.contiguous().view(bs*sl,-1),target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change the embeding size and hidden size to your setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If translating from english -> pivot, no length control\n",
    "encoder_w_att1 = EncoderRNN(en_lang.n_words,300,300,1, vecs=en_lang.vecs).to(device)\n",
    "decoder_w_att1 = AttentionDecoderRNN(intermediate_lang.n_words,300,600, vecs=intermediate_lang.vecs,n_layers=1, len_control=False, attention = True).to(device)\n",
    " # From pivot -> english, use length control\n",
    "encoder_w_att2 = EncoderRNN(intermediate_lang.n_words,300,300,1, vecs=intermediate_lang.vecs).to(device)\n",
    "decoder_w_att2 = AttentionDecoderRNN(en_lang.n_words,300,600, vecs=en_lang.vecs, n_layers=1, len_control=True, attention=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112418 125827\n"
     ]
    }
   ],
   "source": [
    "print(en_lang.n_words, intermediate_lang.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change encoder decoder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_w_att1.load_state_dict(torch.load('./english_italian_sample_encoder1.pth'))\n",
    "decoder_w_att1.load_state_dict(torch.load('./english_italian_sample_decoder1.pth'))\n",
    "encoder_w_att2.load_state_dict(torch.load('./italian_english_sample_encoder1.pth'))\n",
    "decoder_w_att2.load_state_dict(torch.load('./italian_english_sample_decoder1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_out(encoder, decoder, data, lang_first, lang_second, m_type, verbose = False, replace_unk = False, len_cont = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    src_corpus = []\n",
    "\n",
    "    encoder_i = data[0].to(device)\n",
    "    src_len = data[2].to(device)\n",
    "    \n",
    "    tar_len = data[3].to(device)\n",
    "    bs,sl = encoder_i.size()[:2]\n",
    "    en_out,en_hid,en_c = encoder(encoder_i,src_len)\n",
    "    max_src_len_batch = max(src_len).item()\n",
    "    prev_hiddens = en_out\n",
    "    prev_cs = en_c\n",
    "    decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "    prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "    d_out = []\n",
    "    init = True\n",
    "    attention_scores = []\n",
    "    for i in range(sl*2):\n",
    "        out_vocab, prev_output,prev_hiddens, prev_cs, attention_score, tar_len = decoder(decoder_input,prev_output, \\\n",
    "                                                                                prev_hiddens, prev_cs, en_out,\\\n",
    "                                                                                src_len, tar_len, len_cont, init)\n",
    "        init = False\n",
    "        topv, topi = out_vocab.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_out.append(topi.item())\n",
    "        decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        if m_type == 'attention':\n",
    "            attention_scores.append(attention_score.unsqueeze(-1))\n",
    "        if topi.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"True Sentence:\",data[-1])\n",
    "        print(\"Pred Sentence:\", pred_sent)\n",
    "        print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "#     print(score)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half(encoder1, decoder1, data, lang_obj=None, tar_len=None, len_cont = False, prediction = False):\n",
    "\n",
    "    encoder_i = data[0].to(device)\n",
    "#     print(\"ENCODER I SHAPE\", encoder_i)\n",
    "    src_len = data[1].to(device)\n",
    "    if tar_len is not None:\n",
    "        tar_len = torch.tensor([tar_len]).to(device)\n",
    "#     tar_len = None\n",
    "#     if compression_ratio !=None:\n",
    "#         tar_len = torch.tensor([int(compression_ratio * src_len.item())]).to(device)\n",
    "        \n",
    "#     summary = data[-1]\n",
    "    \n",
    "    bs, sl = encoder_i.size()[:2]\n",
    "\n",
    "    en_out, en_hid, en_c = encoder1(encoder_i,src_len)\n",
    "    \n",
    "    prev_hiddens = en_out\n",
    "    prev_cs = en_c\n",
    "    decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "    prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "    \n",
    "    d_out = []\n",
    "    init = True\n",
    "    for i in range(2*sl):\n",
    "        out_vocab, prev_output,prev_hiddens, prev_cs, attention_score, tar_len = decoder1(decoder_input,prev_output, \\\n",
    "                                                                                prev_hiddens, prev_cs, en_out,\\\n",
    "                                                                                src_len, tar_len, init)\n",
    "        init = False\n",
    "        topv, topi = out_vocab.topk(1)\n",
    "        d_out.append(topi.item())\n",
    "        decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "\n",
    "        if topi.item() == EOS_token:\n",
    "            break\n",
    "        \n",
    "#     print(torch.Tensor(d_out).to(device))\n",
    "#     print(\"GOT D_OUT\")\n",
    "    if prediction:\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_obj)\n",
    "        return pred_sent\n",
    "#     , summary\n",
    "    else:\n",
    "        return torch.from_numpy(np.array(d_out)).unsqueeze(0), torch.from_numpy(np.array(len(d_out))).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_new(encoder1, decoder1, encoder2, decoder2, val_dataloader, lang_first, lang_second, compression_ratio=None):\n",
    "    \n",
    "    \n",
    "    encoder1.eval()\n",
    "    decoder1.eval()\n",
    "    encoder2.eval()\n",
    "    decoder2.eval()\n",
    "    \n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    \n",
    "    # Need to keep original src length\n",
    "    # Pass this in along with compression ratio\n",
    "\n",
    "    ii = 0\n",
    "    for data in val_dataloader:\n",
    "        print(ii)\n",
    "        ii += 0\n",
    "        og_src = data[1]\n",
    "        if compression_ratio is not None:\n",
    "            tar_len = int(og_src.item() * compression_ratio)\n",
    "        idx_token2, src_len2 = half(encoder1, decoder1, data)\n",
    "        input_2 = [idx_token2, src_len2]\n",
    "        pred_sent = half(encoder2, decoder2, input_2, lang_first, tar_len, \\\n",
    "                         True, prediction = True)\n",
    "        \n",
    "        \n",
    "        pred_corpus.append(pred_sent)\n",
    "        true_corpus.append(data[-1])\n",
    "    return pred_corpus, true_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c1a1875b013e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_w_att1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_w_att1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-0e723f8a3998>\u001b[0m in \u001b[0;36mhalf\u001b[0;34m(encoder1, decoder1, data, lang_obj, compression_ratio, len_cont, prediction)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0md_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data in dataloader['test']:\n",
    "    half(encoder_w_att1, decoder_w_att1, data, intermediate_lang, prediction=True)\n",
    "    print(i)\n",
    "    i += 1\n",
    "#     print('\\n')\n",
    "#     half(encoder_w_att1, decoder_w_att1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[67539,   213,  4763, 72439,     2, 15086,    37,     5,     2,   138,\n",
      "          1408, 12974, 11417,    21, 41642,     4,    44,    48,  6985,   138,\n",
      "         12974,  2141,    16,     5,   964,     8,  4604,    29,     5,     2,\n",
      "             2, 57084,     6,     1]]), tensor([34]), \"injury leaves kwan 's olympic hopes in limbo\"]\n",
      "TAR LENGTH 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'la comisión de la comisión de UNK UNK , UNK y UNK , UNK , UNK y UNK , UNK , UNK , UNK , UNK , UNK , UNK .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(dataloader['test']))\n",
    "print(data)\n",
    "half(encoder_w_att1, decoder_w_att1, data, intermediate_lang, tar_len=10, prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pred_corpus, true_corpus = validation_new(encoder_w_att1, decoder_w_att1, encoder_w_att2, decoder_w_att2, dataloader['test'], \\\n",
    "              en_lang, intermediate_lang, compression_ratio=COMPRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = pred_corpus\n",
    "test.to_csv('italian_moss.csv', index=False)\n",
    "df_ = pd.read_csv('italian_moss.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>en_len</th>\n",
       "      <th>ref0</th>\n",
       "      <th>ref1</th>\n",
       "      <th>ref2</th>\n",
       "      <th>ref3</th>\n",
       "      <th>ref4</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We believe that the European Union can make an...</td>\n",
       "      <td>['we', 'believe', 'that', 'the', 'european', '...</td>\n",
       "      <td>[53, 509, 17, 5, 30493, 3863, 46, 116, 42, 136...</td>\n",
       "      <td>41</td>\n",
       "      <td>We believe the EU can make an impact , but onl...</td>\n",
       "      <td>but only if it speaks with one voice and if it...</td>\n",
       "      <td>We believe the European Union can make an impa...</td>\n",
       "      <td>We believe the EU can make an impact , but onl...</td>\n",
       "      <td>the UN can make an impact but needs proper too...</td>\n",
       "      <td>we believe that the european union can make an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The figures are worse in Africa and parts of Asia</td>\n",
       "      <td>['the', 'figures', 'are', 'worse', 'in', 'afri...</td>\n",
       "      <td>[5, 2190, 28, 2269, 12, 31342, 7, 961, 9, 4315...</td>\n",
       "      <td>11</td>\n",
       "      <td>It is worse in Africa and parts of Asia</td>\n",
       "      <td>The figures are worse in Africa and parts of Asia</td>\n",
       "      <td>The figures are worse in Africa and Asia</td>\n",
       "      <td>It is worse in Africa and parts of Asia</td>\n",
       "      <td>The numbers are worse in Africa and Asia</td>\n",
       "      <td>the figures are worst enough .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The euro is the tangible result of EMU .</td>\n",
       "      <td>['the', 'euro', 'is', 'the', 'tangible', 'resu...</td>\n",
       "      <td>[5, 12265, 13, 5, 14792, 640, 9, 77210, 6, 1]</td>\n",
       "      <td>10</td>\n",
       "      <td>EMU results in the tangible Euro</td>\n",
       "      <td>The Euro is a tangible result .</td>\n",
       "      <td>Euro is a tangible result of EMU</td>\n",
       "      <td>EMU results in the tangible Euro</td>\n",
       "      <td>The Euro is the result of EMU</td>\n",
       "      <td>the euro is the result .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>['some', 'years', 'ago', ',']</td>\n",
       "      <td>[89, 128, 361, 4, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>Years ago</td>\n",
       "      <td>Some years ago .</td>\n",
       "      <td>Some years ago ,</td>\n",
       "      <td>Years ago ,</td>\n",
       "      <td>some years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have always believed that it is vital that B...</td>\n",
       "      <td>['i', 'have', 'always', 'believed', 'that', 'i...</td>\n",
       "      <td>[202, 36, 256, 2719, 17, 23, 13, 4672, 17, 809...</td>\n",
       "      <td>24</td>\n",
       "      <td>I always that it is vital that Britain plays a...</td>\n",
       "      <td>It is vital that Great Britain play a committe...</td>\n",
       "      <td>I believe that Britain should be a central pla...</td>\n",
       "      <td>I always that it is vital that Britain plays a...</td>\n",
       "      <td>central player in the European Union</td>\n",
       "      <td>i have always believed that the united kingdom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We call for the suspension of the European Uni...</td>\n",
       "      <td>['we', 'call', 'for', 'the', 'suspension', 'of...</td>\n",
       "      <td>[53, 506, 16, 5, 6267, 9, 5, 30493, 3863, 2282...</td>\n",
       "      <td>13</td>\n",
       "      <td>Suspension of the EU fisheries agreement is ca...</td>\n",
       "      <td>We request   suspension of the European Union ...</td>\n",
       "      <td>We call for suspension of the EU fisheries agr...</td>\n",
       "      <td>Suspension of the EU fisheries agreement is ca...</td>\n",
       "      <td>We want to suspend the European Union fisherie...</td>\n",
       "      <td>we are calling for the eu fisheries .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I did n't want to stand out in the crowd any m...</td>\n",
       "      <td>['i', 'did', \"n't\", 'want', 'to', 'stand', 'ou...</td>\n",
       "      <td>[202, 190, 100342, 170, 8, 1316, 65, 12, 5, 31...</td>\n",
       "      <td>14</td>\n",
       "      <td>I did n’t want to stand out any more</td>\n",
       "      <td>I no longer chose to stand out publicly .</td>\n",
       "      <td>I do n't want to me noticed in a crowd .</td>\n",
       "      <td>I did n’t want to stand out any more</td>\n",
       "      <td>I want to blend in with the crowd .</td>\n",
       "      <td>i myself wish to express my support for .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Europol Management Board comprises one rep...</td>\n",
       "      <td>['the', 'europol', 'management', 'board', 'com...</td>\n",
       "      <td>[5, 100020, 895, 955, 6141, 58, 4243, 37, 208,...</td>\n",
       "      <td>26</td>\n",
       "      <td>The Europol Management Board comprises one rep...</td>\n",
       "      <td>The Europol Management Board has one represent...</td>\n",
       "      <td>The Europol Management Board comprises one rep...</td>\n",
       "      <td>The Europol Management Board comprises one rep...</td>\n",
       "      <td>The Europol magagement board comprises one rep...</td>\n",
       "      <td>the europol management committee includes a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The introduction of the euro is a milestone in...</td>\n",
       "      <td>['the', 'introduction', 'of', 'the', 'euro', '...</td>\n",
       "      <td>[5, 4017, 9, 5, 12265, 13, 10, 13143, 12, 5, 6...</td>\n",
       "      <td>22</td>\n",
       "      <td>The Euro is a milestone in the development of ...</td>\n",
       "      <td>The monetary changeover and development of the...</td>\n",
       "      <td>The introduction of the Euro was a milestone .</td>\n",
       "      <td>The Euro is a milestone in the development of ...</td>\n",
       "      <td>the introduction of the Euro is a milestone in...</td>\n",
       "      <td>the introduction of the euro is a progress in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In to replace these chemicals with others that...</td>\n",
       "      <td>['in', 'to', 'replace', 'these', 'chemicals', ...</td>\n",
       "      <td>[12, 8, 2644, 130, 6622, 22, 454, 17, 28, 57, ...</td>\n",
       "      <td>34</td>\n",
       "      <td>This would entail replacing freezers , cooler ...</td>\n",
       "      <td>To replace these chemicals with others that ar...</td>\n",
       "      <td>To replace these chemicals with others more in...</td>\n",
       "      <td>This would entail replacing freezers , cooler ...</td>\n",
       "      <td>replace these chemicals with others   plastic ...</td>\n",
       "      <td>to replace these substances with others who ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  We believe that the European Union can make an...   \n",
       "1  The figures are worse in Africa and parts of Asia   \n",
       "2           The euro is the tangible result of EMU .   \n",
       "3                                   Some years ago ,   \n",
       "4  I have always believed that it is vital that B...   \n",
       "5  We call for the suspension of the European Uni...   \n",
       "6  I did n't want to stand out in the crowd any m...   \n",
       "7  The Europol Management Board comprises one rep...   \n",
       "8  The introduction of the euro is a milestone in...   \n",
       "9  In to replace these chemicals with others that...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  ['we', 'believe', 'that', 'the', 'european', '...   \n",
       "1  ['the', 'figures', 'are', 'worse', 'in', 'afri...   \n",
       "2  ['the', 'euro', 'is', 'the', 'tangible', 'resu...   \n",
       "3                      ['some', 'years', 'ago', ',']   \n",
       "4  ['i', 'have', 'always', 'believed', 'that', 'i...   \n",
       "5  ['we', 'call', 'for', 'the', 'suspension', 'of...   \n",
       "6  ['i', 'did', \"n't\", 'want', 'to', 'stand', 'ou...   \n",
       "7  ['the', 'europol', 'management', 'board', 'com...   \n",
       "8  ['the', 'introduction', 'of', 'the', 'euro', '...   \n",
       "9  ['in', 'to', 'replace', 'these', 'chemicals', ...   \n",
       "\n",
       "                                           en_idized  en_len  \\\n",
       "0  [53, 509, 17, 5, 30493, 3863, 46, 116, 42, 136...      41   \n",
       "1  [5, 2190, 28, 2269, 12, 31342, 7, 961, 9, 4315...      11   \n",
       "2      [5, 12265, 13, 5, 14792, 640, 9, 77210, 6, 1]      10   \n",
       "3                               [89, 128, 361, 4, 1]       5   \n",
       "4  [202, 36, 256, 2719, 17, 23, 13, 4672, 17, 809...      24   \n",
       "5  [53, 506, 16, 5, 6267, 9, 5, 30493, 3863, 2282...      13   \n",
       "6  [202, 190, 100342, 170, 8, 1316, 65, 12, 5, 31...      14   \n",
       "7  [5, 100020, 895, 955, 6141, 58, 4243, 37, 208,...      26   \n",
       "8  [5, 4017, 9, 5, 12265, 13, 10, 13143, 12, 5, 6...      22   \n",
       "9  [12, 8, 2644, 130, 6622, 22, 454, 17, 28, 57, ...      34   \n",
       "\n",
       "                                                ref0  \\\n",
       "0  We believe the EU can make an impact , but onl...   \n",
       "1            It is worse in Africa and parts of Asia   \n",
       "2                   EMU results in the tangible Euro   \n",
       "3                                   Some years ago ,   \n",
       "4  I always that it is vital that Britain plays a...   \n",
       "5  Suspension of the EU fisheries agreement is ca...   \n",
       "6               I did n’t want to stand out any more   \n",
       "7  The Europol Management Board comprises one rep...   \n",
       "8  The Euro is a milestone in the development of ...   \n",
       "9  This would entail replacing freezers , cooler ...   \n",
       "\n",
       "                                                ref1  \\\n",
       "0  but only if it speaks with one voice and if it...   \n",
       "1  The figures are worse in Africa and parts of Asia   \n",
       "2                    The Euro is a tangible result .   \n",
       "3                                          Years ago   \n",
       "4  It is vital that Great Britain play a committe...   \n",
       "5  We request   suspension of the European Union ...   \n",
       "6          I no longer chose to stand out publicly .   \n",
       "7  The Europol Management Board has one represent...   \n",
       "8  The monetary changeover and development of the...   \n",
       "9  To replace these chemicals with others that ar...   \n",
       "\n",
       "                                                ref2  \\\n",
       "0  We believe the European Union can make an impa...   \n",
       "1           The figures are worse in Africa and Asia   \n",
       "2                   Euro is a tangible result of EMU   \n",
       "3                                   Some years ago .   \n",
       "4  I believe that Britain should be a central pla...   \n",
       "5  We call for suspension of the EU fisheries agr...   \n",
       "6           I do n't want to me noticed in a crowd .   \n",
       "7  The Europol Management Board comprises one rep...   \n",
       "8     The introduction of the Euro was a milestone .   \n",
       "9  To replace these chemicals with others more in...   \n",
       "\n",
       "                                                ref3  \\\n",
       "0  We believe the EU can make an impact , but onl...   \n",
       "1            It is worse in Africa and parts of Asia   \n",
       "2                   EMU results in the tangible Euro   \n",
       "3                                   Some years ago ,   \n",
       "4  I always that it is vital that Britain plays a...   \n",
       "5  Suspension of the EU fisheries agreement is ca...   \n",
       "6               I did n’t want to stand out any more   \n",
       "7  The Europol Management Board comprises one rep...   \n",
       "8  The Euro is a milestone in the development of ...   \n",
       "9  This would entail replacing freezers , cooler ...   \n",
       "\n",
       "                                                ref4  \\\n",
       "0  the UN can make an impact but needs proper too...   \n",
       "1           The numbers are worse in Africa and Asia   \n",
       "2                      The Euro is the result of EMU   \n",
       "3                                        Years ago ,   \n",
       "4               central player in the European Union   \n",
       "5  We want to suspend the European Union fisherie...   \n",
       "6                I want to blend in with the crowd .   \n",
       "7  The Europol magagement board comprises one rep...   \n",
       "8  the introduction of the Euro is a milestone in...   \n",
       "9  replace these chemicals with others   plastic ...   \n",
       "\n",
       "                                                pred  \n",
       "0  we believe that the european union can make an...  \n",
       "1                     the figures are worst enough .  \n",
       "2                           the euro is the result .  \n",
       "3                                         some years  \n",
       "4  i have always believed that the united kingdom...  \n",
       "5              we are calling for the eu fisheries .  \n",
       "6          i myself wish to express my support for .  \n",
       "7  the europol management committee includes a re...  \n",
       "8  the introduction of the euro is a progress in ...  \n",
       "9  to replace these substances with others who ar...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"injury leaves kwan 's olympic hopes in limbo\", 'us business attacks tough immigration law', 'gm december sales fall ## percent', 'thousands of croatians celebrate before world cup slalom', \"laura bush <unk> rice to attend sirleaf 's inauguration in liberia\", 'top republican lobbyist pleads guilty to florida fraud', 'somalia rivals in compromise on seat of government', 'portuguese airport workers strike could ground flights on friday', 'maniche renews partnership with mourinho', \"hollywood shores up support for ocean 's thirteen\", 'canada recommends avoiding travel to nepal', 'ford executive sees weaker us auto sales in ####', \"us welcomes chinese journalist 's release highlights freedom concerns\", 'two bankers admit theft from city players', 'unwell sharon admitted to hospital early', 'us hybrid vehicle sales expect to more than triple in seven years', \"korea 's seo headed to dodgers from mets\", 'hollywood starlet lindsay lohan admits bulimia battle', 'us insists soldiers act with restraint to protect civilians', 'us special envoy to korean nuclear talks quits', 'two test positive for bird flu virus in turkey', 'britain urges stronger international support for au in darfur', 'australia backs brazil others for un security council', 'two egyptian guards killed on border with gaza', 'conservatives gain momentum ahead of ruling liberals', 'abn amro hit with second us fine', \"nfl 's bills shake up front office\", 'spurs compound manchester city woes', 'us rate outlook sends dollar to two-month low against euro', 'top us movie groups pick awards nominees as oscars loom', 'pirates ink outfielder burnitz to one-year deal', 'bush says he shares israelis concern over sharon', \"burkina faso 's prime minister resigns\", 'man seeks stay-away order against paris hilton', \"security council to hold ministerial session on africa 's great lakes\", 'israeli leaders unite in prayer for ailing sharon', 'timeline of sharon era', 'human trafficking victims could get right to remain in britain', 'nadal pulls out of sydney international', 'mogilny odd man out as devils welcome back elias', 'olmert to chair emergency israel cabinet meet', 'us actor nick nolte ends <unk>', \"skorea 's consumer confidence tops benchmark ###\", 'denise richards charlie sheen push ahead with divorce', 'hong kong gold opens higher', 'tokyo shares rise #.## percent in morning trade', 'british police seek to arrest moss amid cocaine inquiry', 'key facts about hemorrhagic stroke', 'hong kong shares open higher as rate worries ease', 'inter-korean trade doubles to one billion dollars in ####', 'new vaccines for key <unk> virus shown effective', 'china pushes smaller cars as environmental concerns grow', \"major events in sharon 's life\", 'dollar regains ground in asian trade', \"judge tosses out michael jackson 's memorabilia lawsuit\", 'bush defies congress names defense foreign policy posts', 'oil prices were easier in asian trade', 'agassi withdraws from australian open', 'tokyo shares rise #.## percent in morning trade', 'another botched trade on tokyo stock market', 'scientists locate stem cell which may hold secrets of breast cancer', 'doctors at final stages of sharon surgery', 'intel to invest ### mln dlr in chip plant in vietnam', 'british <unk> <unk> crashes after ## km', \"malaysia probes possible new species of world 's largest flower\", 'india more than china seen as sleeping giant of golf world', 'chinese police angry over new privacy laws', 'matchplay maestro faldo comes in from the cold for royal trophy', 'agassi pulls out of australian open nadal in doubt', 'sharon to remain in surgery for several hours', 'blizzards force evacuation of ##,### people in muslim part of china', 'new zealand sri lanka looks to future goals', 'seven tribesmen shot dead in pakistani tribal area', 'thailand may lift ban on us beef', 'natural disasters claim #,### lives in china in #### four-year high', 'mcgrath reported for obscene behavior in third test', 'australian shares slip on profit-taking', 'third cambodian rights activist charged with defaming pm hun sen', 'netherlands reach hopman cup final after kiefer pulls out', 'dollar regains ground but remains under pressure in asian trade', 'tokyo shares close up #.## percent', 'injured dokic out of canberra international', 'play abandoned for the day in third test due to rain', 'indonesian flood death toll rises to ##', \"lebanon warns of strife with sharon at death 's door\", 'turkey reports second death from bird flu', 'longhorns lasso usc to win first national title in ## years', 'iraqi election final results out within four days', 'olmert chairs emergency israeli cabinet meeting', 'hopman cup boss gives hawk-eye a nod and a wink', 'israeli media declare end to sharon era', 'philippines vows swift resolution of press murders', 'south korea to build up submarine fleet', 'basf prefers friendly takeover of us firm engelhard', 'dubai to bury uae vice president maktoum', 'german retail sales down in november', 'aid groups sound famine disaster warning for northern kenya', \"american skeleton coach wo n't travel with team\", 'cleared <unk> facing another grilling from british swim bosses', 'sharon still in serious condition after surgery']\n"
     ]
    }
   ],
   "source": [
    "print(true_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[    2,   297,  8896,     2,     2,  8023,    73,     4,     2,   122,\n",
      "          2182,     2, 17375,    17,  2426,     9,   101,    36,  4950,   122,\n",
      "             2,  1547,    22,     4,  1677,     5,  2081,    61,     4,     2,\n",
      "          7727,  6740,     6,     1]]), tensor([34]), \"injury leaves kwan 's olympic hopes in limbo\"]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rouge import Rouge \n",
    "# rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = rouge.get_scores(pred_corpus, true_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
