{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from bleu_score import BLEU_SCORE\n",
    "# from models_viet import EncoderRNN, AttentionDecoderRNN, DecoderRNN\n",
    "from load_dataset_viet import *\n",
    "# from define_training_viet import *\n",
    "# import torchtext\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df, val = False):\n",
    "        self.df = df\n",
    "        self.val = val\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx,:]['en_idized']\n",
    "        viet = self.df.iloc[idx,:]['vi_idized']\n",
    "        en_len = self.df.iloc[idx,:]['en_len']\n",
    "        vi_len = self.df.iloc[idx,:]['vi_len']\n",
    "        if self.val:\n",
    "            en_data = self.df.iloc[idx,:]['en_data'].lower()\n",
    "            return [viet,english,vi_len,en_len,en_data]\n",
    "        else:\n",
    "            return [viet,english,vi_len,en_len]\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    MAX_LEN_EN = 48\n",
    "    MAX_LEN_VI = 48\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[3])\n",
    "        vi_len.append(datum[2])\n",
    "    max_batch_length_en = max(en_len)\n",
    "    max_batch_length_vi = max(vi_len)\n",
    "    if max_batch_length_en < MAX_LEN_EN:\n",
    "        MAX_LEN_EN = max_batch_length_en\n",
    "    if max_batch_length_vi < MAX_LEN_VI:\n",
    "        MAX_LEN_VI = max_batch_length_vi\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN_VI:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN_VI]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN_VI - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN_EN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN_EN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN_EN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s2)\n",
    "        vi_data.append(padded_vec_s1)\n",
    "    vi_data = np.array(vi_data)\n",
    "    en_data = np.array(en_data)\n",
    "    vi_len = np.array(vi_len)\n",
    "    en_len = np.array(en_len)\n",
    "#     sorted_vi_len = np.argsort(-vi_len)\n",
    "#     vi_data = vi_data[sorted_vi_len]\n",
    "#     en_data = en_data[sorted_vi_len]\n",
    "#     vi_len = vi_len[sorted_vi_len]\n",
    "#     en_len = en_len[sorted_vi_len]\n",
    "#     print(en_len)\n",
    "    vi_len[vi_len>MAX_LEN_VI] = MAX_LEN_VI\n",
    "    en_len[en_len>MAX_LEN_EN] = MAX_LEN_EN\n",
    "        \n",
    "    return [torch.from_numpy(vi_data), torch.from_numpy(en_data),\n",
    "            torch.from_numpy(vi_len), torch.from_numpy(en_len)]\n",
    "\n",
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)\n",
    "\n",
    "def convert_id_list_2_sent(list_idx, lang_obj):\n",
    "    word_list = []\n",
    "    if type(list_idx) == list:\n",
    "        for i in list_idx:\n",
    "            if i not in set([EOS_token]):\n",
    "                word_list.append(lang_obj.index2word[i])\n",
    "    else:\n",
    "        for i in list_idx:\n",
    "            if i.item() not in set([EOS_token,SOS_token,PAD_IDX]):\n",
    "                word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func_val(batch):\n",
    "    return [torch.from_numpy(np.array(batch[0][0])).unsqueeze(0), torch.from_numpy(np.array(batch[0][1])).unsqueeze(0),\n",
    "            torch.from_numpy(np.array(batch[0][2])).unsqueeze(0), torch.from_numpy(np.array(batch[0][3])).unsqueeze(0),batch[0][4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 48\n",
    "train,val,test,en_lang,vi_lang = train_val_load(48, \"\", '../../machine_translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sorted_batches(train, bs):\n",
    "    batch_samp_list = list(BatchSampler(SequentialSampler(train), bs, drop_last = False))\n",
    "    np.random.shuffle(batch_samp_list)\n",
    "    batch_samp_list_merged = list(itertools.chain(*batch_samp_list))\n",
    "    return train.iloc[batch_samp_list_merged,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = {'train':64,'validate':1, 'train_val':1,'val_train':64, 'test':1}\n",
    "shuffle_dict = {'train':True,'validate':False, 'train_val':False,'val_train':True, 'test':False}\n",
    "# train_used = shuffle_sorted_batches(train_sorted, bs_dict['train'])\n",
    "# train_used = train.iloc[:50]\n",
    "train_used = train.sample(n=100)\n",
    "val_used = val.sample(n=50)\n",
    "# val_used = val.iloc[:20]\n",
    "collate_fn_dict = {'train':vocab_collate_func, 'validate':vocab_collate_func_val,\\\n",
    "                   'train_val':vocab_collate_func_val,'val_train':vocab_collate_func,'test': vocab_collate_func_val}\n",
    "transformed_dataset = {'train': Vietnamese(train_used),\n",
    "                       'validate': Vietnamese(val_used, val = True),\n",
    "                       'train_val':Vietnamese(train.iloc[:50], val = True),\n",
    "                       'val_train':Vietnamese(val_used),\n",
    "                       'test':Vietnamese(test, val= True)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=collate_fn_dict[x],\n",
    "                    shuffle=shuffle_dict[x], num_workers=0) for x in ['train', 'validate', 'train_val','val_train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size,n_layers, rnn_type = 'lstm', device = 'cuda'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(input_size, embed_dim, PAD_IDX)\n",
    "        self.rnn_type =  rnn_type\n",
    "        self.dropout_in = nn.Dropout(p = 0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_size,batch_first=True,bidirectional=True, num_layers = self.n_layers, dropout = 0.2)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = LSTM(embed_dim, hidden_size, batch_first=True,bidirectional=True, num_layers = n_layers,dropout = 0.2)\n",
    "\n",
    "    def forward(self, enc_inp, src_len):\n",
    "        sorted_idx = torch.sort(src_len, descending=True)[1]\n",
    "        orig_idx = torch.sort(sorted_idx)[1]\n",
    "        embedded = self.embedding(enc_inp)\n",
    "        bs = embedded.size(0)\n",
    "        output = self.dropout_in(embedded)\n",
    "        if self.rnn_type == 'gru':\n",
    "            hidden =  self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, hiddden = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, hidden\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            hidden, c = self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, (hiddden, c) = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            c = c[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            c = c.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, c\n",
    "        \n",
    "    def initHidden(self,bs):\n",
    "        if self.rnn_type == 'gru' :\n",
    "            return torch.zeros(self.n_layers*2, bs, self.hidden_size).to(self.device)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            return torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device),torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device)\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, device = 'cuda'):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = Linear(hidden_dim, output_dim, bias = False)\n",
    "        self.l2 = Linear(hidden_dim+output_dim, output_dim, bias =  False)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, hidden, encoder_outs, src_lens):\n",
    "        ''' hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "        \n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_len'''\n",
    "        x = self.l1(hidden)\n",
    "        att_score = (encoder_outs.transpose(0,1) * x.unsqueeze(0)).sum(dim = 2)\n",
    "        seq_mask = sequence_mask(src_lens, max_len = max(src_lens).item(), device = self.device).transpose(0,1)\n",
    "        masked_att = seq_mask*att_score\n",
    "        masked_att[masked_att==0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0)\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0,1)).sum(dim=0)\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1)))\n",
    "        return x, attn_scores\n",
    "        \n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size, n_layers = 1, attention = True, device = 'cuda'):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_output_size = hidden_size\n",
    "        self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.att_layer = Attention_Module(self.hidden_size, encoder_output_size,self.device) if attention else None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=self.hidden_size + embed_dim if ((layer == 0) and attention) else embed_dim if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(self.n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.no_len_control_ll = nn.Linear(1024, 1024)\n",
    "        self.len_control_ll = nn.Linear(1034, 1024)\n",
    "        self.len_params = nn.Parameter(torch.rand(1,10))\n",
    "        \n",
    "    def forward(self, dec_input, context_vector, prev_hiddens, prev_cs, encoder_outputs, src_len, len_control=False, init=False):\n",
    "        bsz = dec_input.size(0)\n",
    "        output = self.embedding(dec_input)\n",
    "        output = self.dropout(output)\n",
    "        if init:\n",
    "            if len_control:\n",
    "                prev_hiddens = self.len_control_ll(torch.cat((torch.mean(prev_hiddens, dim=1), \\\n",
    "                                                          self.len_params.repeat(bsz, 1)),dim=1)).unsqueeze(0)\n",
    "            else:\n",
    "                prev_hiddens = self.no_len_control_ll(torch.mean(prev_hiddens, dim=1)).unsqueeze(0)\n",
    "            \n",
    "#         print(\"decoder\", prev_hiddens.size(), len_control)\n",
    "#         print(dec_input.size(), prev_hiddens.size(), context_vector.size(), prev_cs.size(), len_control, init)\n",
    "        if self.att_layer is not None:\n",
    "            cated_input = torch.cat([output.squeeze(1),context_vector], dim = 1)\n",
    "        else:\n",
    "            cated_input = output.squeeze(1)\n",
    "        new_hiddens = []\n",
    "        new_cs = []\n",
    "        for i, rnn in enumerate(self.layers):\n",
    "            hidden, c = rnn(cated_input, (prev_hiddens[i], prev_cs[i]))\n",
    "            cated_input = self.dropout(hidden)\n",
    "            new_hiddens.append(hidden.unsqueeze(0))\n",
    "            new_cs.append(c.unsqueeze(0))\n",
    "        new_hiddens = torch.cat(new_hiddens, dim = 0)\n",
    "        new_cs = torch.cat(new_cs, dim = 0)\n",
    "\n",
    "        # apply attention using the last layer's hidden state\n",
    "        if self.att_layer is not None:\n",
    "            out, attn_score = self.att_layer(hidden, encoder_outputs, src_len)\n",
    "        else:\n",
    "            out = hidden\n",
    "            attn_score = None\n",
    "        context_vec = out\n",
    "        out = self.dropout(out)\n",
    "        out_vocab = self.softmax(self.fc_out(out))\n",
    "\n",
    "        return out_vocab, context_vec, new_hiddens, new_cs, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTM(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTM(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True, dropout=0):\n",
    "    \"\"\"Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features, bias=bias)\n",
    "    m.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        m.bias.data.uniform_(-0.1, 0.1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None, device = 'cuda'):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.max().item()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size,1])\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return (seq_range_expand < seq_length_expand).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(encoder, decoder, data_en, data_de, src_len, tar_len, rand_num = 0.95, len_cont = False, val = False):\n",
    "    if not val:\n",
    "        use_teacher_forcing = True if random.random() < rand_num else False\n",
    "        bss = data_en.size(0)\n",
    "        en_out, en_hid, en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_out\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        init = True\n",
    "        if use_teacher_forcing:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output, prev_hiddens, prev_cs, attention_score = decoder(decoder_input, prev_output, \\\n",
    "                                                                                        prev_hiddens, prev_cs, en_out,\\\n",
    "                                                                                        src_len, len_cont, init)\n",
    "                init = False\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                decoder_input = data_de[:,i].view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        else:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output, prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len, len_cont, init)\n",
    "                init = False\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                topv, topi = out_vocab.topk(1)\n",
    "                decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        bss = data_en.size(0)\n",
    "        en_out,en_hid,en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        d_out = []\n",
    "        for i in range(max_tar_len_batch):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "            d_out.append(out_vocab.unsqueeze(-1))\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cel_loss(input,target,nll):\n",
    "    input = input.transpose(1,2)\n",
    "    bs, sl = input.size()[:2]\n",
    "    return nll(input.contiguous().view(bs*sl,-1),target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-94577690276b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-94577690276b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    new validation torch.Size([1, 1]) torch.Size([1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 13, 1024])\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "new validation torch.Size([1, 1]) torch.Size([1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 13, 1024])\n",
    "sec dec torch.Size([1, 1]) torch.Size([1, 1024]) torch.Size([1, 26, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 26, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_new(encoder, decoder, encoder2, decoder2, val_dataloader, lang_en, lang_vi, m_type, verbose = False, replace_unk = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    src_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    attention_scores_for_all_val = []\n",
    "    for data in val_dataloader:\n",
    "        \n",
    "        encoder_i = data[1].to(device)\n",
    "        src_len = data[3].to(device)\n",
    "        \n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        \n",
    "        en_out,en_hid,en_c = encoder(encoder_i, src_len)\n",
    "        \n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        \n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "        prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "        \n",
    "        d_out = []\n",
    "        attention_scores = []\n",
    "        init = True\n",
    "#         print(\"Validation new first decoder\")\n",
    "#         print(\"new validation\", decoder_input.size(), prev_output.size(), prev_hiddens.size(),prev_cs.size(),en_out.size())\n",
    "        for i in range(sl*2):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens, prev_cs, en_out,\\\n",
    "                                                                                    src_len, False, init)\n",
    "            init = False\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "            d_out.append(topi.item())\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             if m_type == 'attention':\n",
    "#                 attention_scores.append(attention_score.unsqueeze(-1))\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "        \n",
    "        d_out = torch.tensor(d_out).view(1,-1).to(device)\n",
    "        \n",
    "        encoder_i2 = d_out\n",
    "        src_len2 = torch.tensor([encoder_i2.size(1)]).to(device)\n",
    "        bs, s2 = encoder_i2.size()\n",
    "        \n",
    "        en_out2, en_hid2, en_c2 = encoder2(encoder_i2, src_len2)\n",
    "        \n",
    "        prev_hiddens2 = en_out2\n",
    "        prev_cs2 = en_c2\n",
    "        decoder_input2 = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "        prev_output2 = torch.zeros((bs, en_out2.size(-1))).to(device)\n",
    "        \n",
    "        dec2_out = []\n",
    "        init = True\n",
    "#         print(\"validation new second decoder\")\n",
    "#         print(\"sec dec\",decoder_input2.size(), prev_output2.size(), prev_hiddens2.size(), prev_cs2.size(),en_out2.size())\n",
    "        for i in range(s2*2):\n",
    "            out_vocab2, prev_output2, prev_hiddens2, prev_cs2, attention_scores2 = decoder(decoder_input2, prev_output2, \\\n",
    "                                                                                     prev_hiddens2, prev_cs2, en_out2, \\\n",
    "                                                                                     src_len2, True, init)\n",
    "            init = False            \n",
    "            topv, topi = out_vocab2.topk(1)\n",
    "            dec2_out.append(topi.item())\n",
    "            decoder_input2 = topi.squeeze().detach().view(-1,1)\n",
    "            if topi.item == EOS_token:\n",
    "                break\n",
    "                \n",
    "        d_out = torch.tensor(dec2_out).to(device)\n",
    "        \n",
    "        \n",
    "#         true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "#         true_corpus.append(true_sent)\n",
    "        if replace_unk:\n",
    "            true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "            true_corpus.append(true_sent)\n",
    "        else:\n",
    "            true_corpus.append(data[-1])\n",
    "        src_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "        src_corpus.append(src_sent)\n",
    "#         print(d_out)\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "        pred_corpus.append(pred_sent)\n",
    "#         print(\"src : \",src_sent)\n",
    "#         print(\"Pred : \",pred_sent)\n",
    "        if m_type == 'attention':\n",
    "            attention_scores = torch.cat(attention_scores, dim = -1)\n",
    "            attention_scores_for_all_val.append(attention_scores)\n",
    "        if verbose:\n",
    "            print(\"True Sentence:\",data[-1])\n",
    "            print(\"Pred Sentence:\", pred_sent)\n",
    "            print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return score, attention_scores_for_all_val, pred_corpus, src_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_compression(encoder_optimizer, decoder_optimizer, encoder_optimizer2, decoder_optimizer2, \\\n",
    "                            encoder, decoder, encoder2, \\\n",
    "                            decoder2, loss_fun, m_type, dataloader, en_lang, vi_lang,\\\n",
    "                num_epochs=60, val_every = 1, train_bleu_every = 10,clip = 0.1, rm = 0.8, enc_scheduler = None,\\\n",
    "               dec_scheduler = None, enc_scheduler2 = None, dec_scheduler2 = None, enc_dec_fn = encode_decode, val_fn = validation_new):\n",
    "    best_score = 0\n",
    "    best_bleu = 0\n",
    "    loss_hist = {'train': [], 'val_train': []}\n",
    "    bleu_hist = {'train': [], 'validate': []}\n",
    "    best_encoder_wts = None\n",
    "    best_decoder_wts = None\n",
    "    best_encoder_wts2 = None\n",
    "    best_decoder_wts2 = None\n",
    "    phases = ['train']\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(phases):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "                encoder2.train()\n",
    "                decoder2.train()\n",
    "            else:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "                encoder2.eval()\n",
    "                decoder2.eval()\n",
    "                \n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                encoder_optimizer2.zero_grad()\n",
    "                decoder_optimizer2.zero_grad()\n",
    "\n",
    "                encoder_i = data[1].to(device)\n",
    "                decoder_i = data[0].to(device)\n",
    "                src_len = data[3].to(device)\n",
    "                tar_len = data[2].to(device)\n",
    "                \n",
    "                out = enc_dec_fn(encoder, decoder, encoder_i, decoder_i, src_len, tar_len, rand_num = rm, len_cont=False, val = False )\n",
    "                \n",
    "                topi = out.argmax(dim=1)\n",
    "                \n",
    "                encoder2_i = topi\n",
    "                decoder2_i = data[1].to(device)\n",
    "                src_len2 = data[2].to(device)\n",
    "                tar_len2 = data[3].to(device)\n",
    "            \n",
    "#                 print(out.size(),encoder_i.size(), encoder2_i.size(), decoder_i.size(), decoder2_i.size())\n",
    "                out2 = enc_dec_fn(encoder2, decoder2, encoder2_i, decoder2_i, src_len2, tar_len2, rand_num = rm, len_cont= True, val = False )\n",
    "                \n",
    "#                 print(\"out size = \", out.size(), topi.size(), out2.size())\n",
    "                N = decoder_i.size(0)\n",
    "                loss = loss_fun(out.float(), decoder_i.long()) + loss_fun(out2.float(), decoder2_i.long())\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(encoder2.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(decoder2.parameters(), clip)\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    encoder_optimizer2.step()\n",
    "                    decoder_optimizer2.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / total \n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            print(\"epoch {} {} loss = {}, time = {}\".format(epoch, phase, epoch_loss,\n",
    "                                                                           time.time() - start))\n",
    "        if (enc_scheduler is not None) and (dec_scheduler is not None):\n",
    "            enc_scheduler.step(loss_hist['train'][-1])\n",
    "            dec_scheduler.step(loss_hist['train'][-1])\n",
    "            enc_scheduler2.step(loss_hist['train'][-1])\n",
    "            dec_scheduler2.step(loss_hist['train'][-1])\n",
    "            \n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score, _,_,_ = val_fn(encoder, decoder, encoder2, decoder2, dataloader['validate'],\\\n",
    "                                            en_lang,vi_lang,m_type, verbose=False, \\\n",
    "                                            replace_unk=True)\n",
    "            \n",
    "            bleu_hist['validate'].append(val_bleu_score)\n",
    "            print(\"validation BLEU = \", val_bleu_score)\n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                best_encoder_wts = encoder.state_dict()\n",
    "                best_decoder_wts = decoder.state_dict()\n",
    "                best_encoder_wts2 = encoder2.state_dict()\n",
    "                best_decoder_wts2 = decoder2.state_dict()\n",
    "                \n",
    "        print('='*50)\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "    encoder2.load_state_dict(best_encoder_wts2)\n",
    "    decoder2.load_state_dict(best_decoder_wts2)\n",
    "    \n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "    return encoder, decoder, encoder2, decoder2, loss_hist, bleu_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoder_w_att = EncoderRNN(en_lang.n_words, 512,512, 1).to(device)\n",
    "encoder_w_att_2 = EncoderRNN(vi_lang.n_words, 512,512, 1).to(device)\n",
    "\n",
    "decoder_w_att = AttentionDecoderRNN(vi_lang.n_words, 512,1024,n_layers=1, attention = True).to(device)\n",
    "decoder_w_att_2 = AttentionDecoderRNN(en_lang.n_words, 512,1024,n_layers=1, attention = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.Adam(encoder_wo_att.parameters(), lr = 5e-3)\n",
    "# decoder_optimizer = optim.Adam(decoder_wo_att.parameters(), lr = 5e-3)\n",
    "encoder_optimizer = optim.SGD(encoder_w_att.parameters(), lr=0.1,nesterov=True, momentum = 0.99)\n",
    "encoder_optimizer2 = optim.SGD(encoder_w_att_2.parameters(), lr=0.1,nesterov=True, momentum = 0.99)\n",
    "\n",
    "decoder_optimizer = optim.SGD(decoder_w_att.parameters(), lr=0.1,nesterov=True, momentum = 0.99)\n",
    "decoder_optimizer2 = optim.SGD(decoder_w_att_2.parameters(), lr=0.1,nesterov=True, momentum = 0.99)\n",
    "\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "enc_scheduler2 = ReduceLROnPlateau(encoder_optimizer2, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "dec_scheduler2 = ReduceLROnPlateau(decoder_optimizer2, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# enc_scheduler = None\n",
    "# enc_scheduler2 = None\n",
    "# dec_scheduler = None\n",
    "# dec_scheduler2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 10.609091453552246, time = 2.3843555450439453\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 1 train loss = 10.57917121887207, time = 2.3345425128936768\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 2 train loss = 10.566456565856933, time = 2.3343799114227295\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 3 train loss = 10.542923278808594, time = 2.2836644649505615\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 4 train loss = 10.50886402130127, time = 2.3286988735198975\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 5 train loss = 10.481978912353515, time = 2.3347620964050293\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 6 train loss = 10.462536773681641, time = 2.336409091949463\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 7 train loss = 10.462386932373047, time = 2.269564151763916\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 8 train loss = 10.428451690673828, time = 2.3299927711486816\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 9 train loss = 10.40544075012207, time = 2.3351430892944336\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 10 train loss = 10.385713195800781, time = 2.334787130355835\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 11 train loss = 10.38613021850586, time = 2.2695600986480713\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 12 train loss = 10.36633804321289, time = 2.3336572647094727\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 13 train loss = 10.363598976135254, time = 2.333501100540161\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 14 train loss = 10.36591022491455, time = 2.2547812461853027\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 15 train loss = 10.352271118164062, time = 2.333329200744629\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 16 train loss = 10.337114791870118, time = 2.3380484580993652\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 17 train loss = 10.35338768005371, time = 2.281446695327759\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 18 train loss = 10.340664978027343, time = 2.3308565616607666\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 19 train loss = 10.3276167678833, time = 2.3339767456054688\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 20 train loss = 10.337860870361329, time = 2.331608772277832\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 21 train loss = 10.398215103149415, time = 2.3442680835723877\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 22 train loss = 10.327504730224609, time = 2.333873748779297\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 23 train loss = 10.32858512878418, time = 2.331432342529297\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 24 train loss = 10.333828964233398, time = 2.3421032428741455\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 25 train loss = 10.353247604370118, time = 2.3385722637176514\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 26 train loss = 10.353580474853516, time = 2.040724277496338\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 27 train loss = 10.321725807189942, time = 2.3625600337982178\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 28 train loss = 10.332640533447266, time = 2.156113624572754\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 29 train loss = 10.37429817199707, time = 2.1311898231506348\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 30 train loss = 10.335068435668946, time = 2.3346548080444336\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 31 train loss = 10.319049224853515, time = 2.3308026790618896\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 32 train loss = 10.337009048461914, time = 2.278055429458618\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 33 train loss = 10.310435333251952, time = 2.1145782470703125\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 34 train loss = 10.391636047363281, time = 2.2779734134674072\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 35 train loss = 10.322433738708495, time = 2.3311052322387695\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 36 train loss = 10.325583381652832, time = 2.332486152648926\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 37 train loss = 10.329707221984863, time = 2.332293748855591\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 38 train loss = 10.323649253845215, time = 2.3284494876861572\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 39 train loss = 10.352060241699219, time = 2.213562250137329\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 40 train loss = 10.327438735961914, time = 2.3343303203582764\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 41 train loss = 10.324327850341797, time = 2.3319132328033447\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 42 train loss = 10.324170913696289, time = 2.3326754570007324\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 43 train loss = 10.355964279174804, time = 2.338239908218384\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 44 train loss = 10.32350772857666, time = 2.3328380584716797\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 45 train loss = 10.345477142333984, time = 2.3367786407470703\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 46 train loss = 10.342905731201173, time = 2.2242555618286133\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 47 train loss = 10.333430786132812, time = 2.3306782245635986\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 48 train loss = 10.402842636108398, time = 2.3428823947906494\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 49 train loss = 10.333262519836426, time = 2.335324287414551\n",
      "validation BLEU =  0.0\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-83dfef8d70fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_w_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_w_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_w_att_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_w_att_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_hist\u001b[0m \u001b[0;34m=\u001b[0m                         \u001b[0mtrain_model_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m                                                             \u001b[0mencoder_optimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_w_att\u001b[0m\u001b[0;34m,\u001b[0m                                                 \u001b[0mdecoder_w_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_w_att_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_w_att_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m                                            \u001b[0;34m\"non_attention\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m                                           \u001b[0menc_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_scheduler\u001b[0m\u001b[0;34m,\u001b[0m                                                 \u001b[0menc_scheduler2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_scheduler2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_scheduler2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_scheduler2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-f9eaf79ee7cf>\u001b[0m in \u001b[0;36mtrain_model_compression\u001b[0;34m(encoder_optimizer, decoder_optimizer, encoder_optimizer2, decoder_optimizer2, encoder, decoder, encoder2, decoder2, loss_fun, m_type, dataloader, en_lang, vi_lang, num_epochs, val_every, train_bleu_every, clip, rm, enc_scheduler, dec_scheduler, enc_scheduler2, dec_scheduler2, enc_dec_fn, val_fn)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_encoder_wts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_decoder_wts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mencoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_encoder_wts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "encoder_w_att, decoder_w_att, encoder_w_att_2, decoder_w_att_2, loss_hist, acc_hist = \\\n",
    "                        train_model_compression(encoder_optimizer, decoder_optimizer, \\\n",
    "                                                            encoder_optimizer2, decoder_optimizer2, encoder_w_att, \\\n",
    "                                                decoder_w_att, encoder_w_att_2, decoder_w_att_2, criterion,\\\n",
    "                                            \"non_attention\", dataloader,en_lang, vi_lang, num_epochs = 50, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler, \\\n",
    "                                                enc_scheduler2 = enc_scheduler2, dec_scheduler2 = dec_scheduler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RuntimeError: Input batch size 1 doesn't match hidden[0] batch size 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
